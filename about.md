
The project shows how to remove stop words from a text file using the NLTK package in Python, as well as demonstrates how to add more words to the NLTK stop-word list. This process removes commonly used words that usually interfere with textual analysis. By running this program, the data is cleaned and in a better form for analysis. 

This repository includes, a README.md file explaining the project, a file with the Python script, my initial text file, and a text file with the stop words removed.

The idea for this project came about with new developments some of my past research. During my undergrad, I wrote a thesis about the origins of the British welfare state. Most of the primary sources came from parliamentary debates that took place during WWII. I found and downloaded these debates, in a pdf form as this was the only option, to use in my research. Analysis was slow going as pdf doesn’t allow for too much manipulation. I also was not well versed in digital tools so the process of digital text analysis was unknown to me. This past Fall, I found these debates available for download as plain text files. At that point I was familiar with text analysis and interested in learning more about prepping the data for analysis. 

This semester I spend some time researching textual analysis and methods for cleaning and prepping the data for investigation. I specifically explored the NLTK library and its capabilities. At first, I did not understand what the code was doing or how it was doing it, but after participating in the GC DRI and our lessons with Python in class, I now have a better understanding of how it is running. 

As a first step toward text analysis, I needed to clean my text files to make them easier to manipulate. There are quite a few things I need to do to prep the data, so I thought removing stop words was a good place to start. The basic script for removing stop words was pretty standard across various ‘help pages’ on the web. I searched around a little more to find how to run a Python script on a plain text file as many examples only had strings within the script to show how the code worked. Additionally, I wanted to know how to add more words to the NLTK stop-word list. It took me some time to find the right code that worked within my Python script. 

When I first ran the script and it worked, I was completely surprised. I thought for sure an error would occur, but it ran smoothly and produced a new plain text file with the stop words removed. I believe my practice with Python in class and during my journal hours helped me understand the code and get it working the first time through. 

My next steps include running additional scripts to remove more stop words as well as removing punctuation. Eventually I will move onto the other debate plain text files and prep them for analysis. I hope to sometime this summer start using functions from the NLTK library to run textual analysis on these text files.


